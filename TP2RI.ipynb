{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata as ud\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "from string import punctuation\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer, ISRIStemmer\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parameters's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeofclass = 1\n",
    "lang = \"en\"\n",
    "n = 2\n",
    "c_path = \"./corpora\"\n",
    "c_path = os.path.abspath(c_path)\n",
    "# typeofclass = int(\n",
    "#     input(\"to use word based classification use 1, for character based one use 2: \"))\n",
    "# lang = input(\"enter en for english corpus, ar for the arabic one: \")\n",
    "# n = int(input(\"Enter the number of ngrams: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure tokenizers, stemmers, stopwords based on parameters's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if lang == \"en\":\n",
    "    c_path += \"/english\"\n",
    "    stopwords = stopwords.words(\"english\")\n",
    "    stemer = PorterStemmer()\n",
    "else:\n",
    "    c_path += \"/arabic\"\n",
    "    stopwords = stopwords.words(\"arabic\")\n",
    "    stemer = ISRIStemmer()\n",
    "\n",
    "\n",
    "stoplist = set(stopwords + list(punctuation))\n",
    "\n",
    "retoken = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "pattern = re.compile(r'^([0-9]+\\w*)')\n",
    "if lang == \"ar\":\n",
    "    pattern = re.compile(r'^([0-9]+\\w*)|[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = ''.join(c for c in text if not ud.category(c).startswith('P'))\n",
    "    text = ' '.join(retoken.tokenize(text))\n",
    "    tokens = [\n",
    "        token for token in nltk.word_tokenize(\n",
    "            text)\n",
    "        if token.lower() not in stoplist\n",
    "        and\n",
    "        not token.lower().isdigit()\n",
    "        and\n",
    "        pattern.match(token) == None\n",
    "    ]\n",
    "    tokens = [stemer.stem(w) for w in tokens]\n",
    "    if typeofclass == 2:\n",
    "        tokens = [c for w in tokens for c in w]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading corpus's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(c_path, \".*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting ngrams -> frequency dictionary for each corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ngrams = []\n",
    "filesids = corpus._fileids\n",
    "dictNgrams = {}\n",
    "for f in filesids:\n",
    "    file_sentences = corpus.sents(f)\n",
    "    corpus_ngrams_perfile = []\n",
    "    for sent in file_sentences:\n",
    "        sent_words = preprocess_text(\" \".join(sent))\n",
    "        sent_n_grams = ngrams(sent_words, n)\n",
    "\n",
    "        corpus_ngrams.append(list(sent_n_grams))\n",
    "        corpus_ngrams_perfile += [w for w in corpus_ngrams[-1]]\n",
    "\n",
    "    frq_dist_f = nltk.FreqDist(corpus_ngrams_perfile)\n",
    "    temp_dict = {\" \".join(k): v for k, v in dict(frq_dist_f).items()}\n",
    "    temp_dict = collections.OrderedDict(sorted(temp_dict.items()))\n",
    "    dictNgrams[f] = temp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform ngrams's dictionary into a pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(dictNgrams)\n",
    "tf_idf = tf_idf.sort_index()\n",
    "tf_idf = tf_idf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.to_csv(\"ngrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating idf and Tf.idf score for each corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61924/3387467829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     len(row)/(len(row)-len([w for w in row if w == 0]))), axis=1)\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilesids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "tf_idf[\"idf\"] = tf_idf.apply(lambda row: math.log10(\n",
    "    len(row)/(len(row)-len([w for w in row if w == 0]))), axis=1)\n",
    "for f in filesids:\n",
    "    tf_idf[f] = (1 + np.log10(tf_idf[f]))*tf_idf[\"idf\"]\n",
    "tf_idf = tf_idf.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Tf-Idf into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.to_csv(\"tf_idf.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d1bc2f73409776b7446c973d16746829351ceb8421732852f5d01cd72ff89a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
