{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata as ud\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "from string import punctuation\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer, ISRIStemmer\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parameters's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeofclass = 1\n",
    "lang = \"en\"\n",
    "n = 1\n",
    "c_path = \"./corpora\"\n",
    "c_path = os.path.abspath(c_path)\n",
    "# typeofclass = int(\n",
    "#     input(\"to use word based classification use 1, for character based one use 2: \"))\n",
    "# lang = input(\"enter en for english corpus, ar for the arabic one: \")\n",
    "# n = int(input(\"Enter the number of ngrams: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure tokenizers, stemmers, stopwords based on parameters's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if lang == \"en\":\n",
    "    c_path += \"/english\"\n",
    "    stopwords = stopwords.words(\"english\")\n",
    "    stemer = PorterStemmer()\n",
    "else:\n",
    "    c_path += \"/arabic\"\n",
    "    stopwords = stopwords.words(\"arabic\")\n",
    "    stemer = ISRIStemmer()\n",
    "\n",
    "\n",
    "stoplist = set(stopwords + list(punctuation))\n",
    "\n",
    "retoken = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "pattern = re.compile(r'^([0-9]+\\w*)')\n",
    "if lang == \"ar\":\n",
    "    pattern = re.compile(r'^([0-9]+\\w*)|[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = ''.join(c for c in text if not ud.category(c).startswith('P'))\n",
    "    text = ' '.join(retoken.tokenize(text))\n",
    "    tokens = [\n",
    "        token for token in nltk.word_tokenize(\n",
    "            text)\n",
    "        if token.lower() not in stoplist\n",
    "        and\n",
    "        not token.lower().isdigit()\n",
    "        and\n",
    "        pattern.match(token) == None\n",
    "    ]\n",
    "    tokens = [stemer.stem(w) for w in tokens]\n",
    "    if typeofclass == 2:\n",
    "        tokens = [c for w in tokens for c in w]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading corpus's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(c_path, \".*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting ngrams -> frequency dictionary for each corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ngrams = []\n",
    "filesids = corpus._fileids\n",
    "dictNgrams = {}\n",
    "for f in filesids:\n",
    "    file_sentences = corpus.sents(f)\n",
    "    corpus_ngrams_perfile = []\n",
    "    for sent in file_sentences:\n",
    "        sent_words = preprocess_text(\" \".join(sent))\n",
    "        sent_n_grams = ngrams(sent_words, n)\n",
    "\n",
    "        corpus_ngrams.append(list(sent_n_grams))\n",
    "        corpus_ngrams_perfile += [w for w in corpus_ngrams[-1]]\n",
    "\n",
    "    frq_dist_f = nltk.FreqDist(corpus_ngrams_perfile)\n",
    "    temp_dict = {\" \".join(k): v for k, v in dict(frq_dist_f).items()}\n",
    "    temp_dict = collections.OrderedDict(sorted(temp_dict.items()))\n",
    "    dictNgrams[f] = temp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform ngrams's dictionary into a pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(dictNgrams)\n",
    "tf_idf = tf_idf.sort_index()\n",
    "tf_idf = tf_idf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.to_csv(\"ngrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating idf and Tf.idf score for each corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dex/.local/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tf_idf[\"idf\"] = tf_idf.apply(lambda row: math.log10(\n",
    "    len(row)/(len(row)-len([w for w in row if w == 0]))), axis=1)\n",
    "for f in filesids:\n",
    "    tf_idf[f] = (1 + np.log10(tf_idf[f]))*tf_idf[\"idf\"]\n",
    "tf_idf = tf_idf.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corp1.txt</th>\n",
       "      <th>corp10.txt</th>\n",
       "      <th>corp2.txt</th>\n",
       "      <th>corp3.txt</th>\n",
       "      <th>corp4.txt</th>\n",
       "      <th>corp5.txt</th>\n",
       "      <th>corp6.txt</th>\n",
       "      <th>corp7.txt</th>\n",
       "      <th>corp8.txt</th>\n",
       "      <th>corp9.txt</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abl find</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aboard next</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absenc year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access problem</th>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord research</th>\n",
       "      <td>0.903997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>0.564271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord south</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord studi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>across tropic</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>across world</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>0.564271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action need</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 corp1.txt  corp10.txt  corp2.txt  corp3.txt  corp4.txt  \\\n",
       "abl find          0.000000    0.000000        0.0   0.000000   0.000000   \n",
       "aboard next       0.000000    0.740363        0.0   0.000000   0.000000   \n",
       "absenc year       0.000000    0.740363        0.0   0.000000   0.000000   \n",
       "access problem    0.740363    0.000000        0.0   0.000000   0.000000   \n",
       "accord research   0.903997    0.000000        0.0   0.000000   0.000000   \n",
       "accord south      0.000000    0.000000        0.0   0.439333   0.439333   \n",
       "accord studi      0.000000    0.000000        0.0   0.000000   0.000000   \n",
       "across tropic     0.000000    0.000000        0.0   0.000000   0.000000   \n",
       "across world      0.000000    0.000000        0.0   0.000000   0.000000   \n",
       "action need       0.000000    0.000000        0.0   0.000000   0.000000   \n",
       "\n",
       "                 corp5.txt  corp6.txt  corp7.txt  corp8.txt  corp9.txt  \\\n",
       "abl find          0.000000        0.0        0.0   0.740363   0.000000   \n",
       "aboard next       0.000000        0.0        0.0   0.000000   0.000000   \n",
       "absenc year       0.000000        0.0        0.0   0.000000   0.000000   \n",
       "access problem    0.000000        0.0        0.0   0.000000   0.000000   \n",
       "accord research   0.000000        0.0        0.0   0.000000   0.564271   \n",
       "accord south      0.439333        0.0        0.0   0.000000   0.000000   \n",
       "accord studi      0.000000        0.0        0.0   0.000000   0.740363   \n",
       "across tropic     0.000000        0.0        0.0   0.000000   0.740363   \n",
       "across world      0.000000        0.0        0.0   0.564271   0.564271   \n",
       "action need       0.000000        0.0        0.0   0.000000   0.740363   \n",
       "\n",
       "                      idf  \n",
       "abl find         0.740363  \n",
       "aboard next      0.740363  \n",
       "absenc year      0.740363  \n",
       "access problem   0.740363  \n",
       "accord research  0.564271  \n",
       "accord south     0.439333  \n",
       "accord studi     0.740363  \n",
       "across tropic    0.740363  \n",
       "across world     0.564271  \n",
       "action need      0.740363  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Tf-Idf into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.to_csv(\"tf_idf.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d1bc2f73409776b7446c973d16746829351ceb8421732852f5d01cd72ff89a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
